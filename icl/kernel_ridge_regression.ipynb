{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1",
   "metadata": {},
   "source": [
    "# Kernel Regression as In-Context Learning\n",
    "\n",
    "When a transformer receives examples $(x_1, y_1), \\ldots, (x_n, y_n)$ in its prompt and predicts $y$ for a new query $x_q$ — **without any weight update** — it is doing *in-context learning* (ICL).\n",
    "\n",
    "This notebook shows that ICL is structurally **kernel regression**:\n",
    "\n",
    "| In-Context Learning | Kernel Regression |\n",
    "|---|---|\n",
    "| Context examples $(x_i, y_i)$ | Training set |\n",
    "| Query $x_q$ | Test point |\n",
    "| Attention weights $\\text{softmax}(x_q \\cdot x_i)$ | Kernel similarities $k(x_q, x_i)$ |\n",
    "| Output = weighted sum of $y_i$ | Prediction = weighted sum of $y_i$ |\n",
    "| No weight update | No weight update |\n",
    "\n",
    "We'll build this connection in three steps:\n",
    "\n",
    "1. **Ridge → Dual form** — rewrite ridge regression so data appears only through dot products\n",
    "2. **Dual → Kernel regression** — replace dot products with a kernel (the kernel trick)\n",
    "3. **Kernel regression = ICL** — show a single attention head is literally kernel regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float64\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1",
   "metadata": {},
   "source": [
    "## Step 1: Ridge Regression — Primal and Dual\n",
    "\n",
    "Ridge regression minimises $\\|Xw - y\\|^2 + \\lambda\\|w\\|^2$. Two equivalent solutions:\n",
    "\n",
    "| | Primal | Dual |\n",
    "|---|---|---|\n",
    "| **Solve for** | $w \\in \\mathbb{R}^d$ | $\\alpha \\in \\mathbb{R}^n$ |\n",
    "| **Formula** | $w = (X^\\top X + \\lambda I_d)^{-1} X^\\top y$ | $\\alpha = (XX^\\top + \\lambda I_n)^{-1} y$ |\n",
    "| **Predict** | $\\hat{y} = X_\\text{new}\\, w$ | $\\hat{y} = X_\\text{new} X^\\top \\alpha$ |\n",
    "| **Inverts** | $(d \\times d)$ matrix | $(n \\times n)$ matrix |\n",
    "\n",
    "They are the same (Woodbury identity). But in the dual form, $X$ only appears as dot products $XX^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgePrimal:\n",
    "    \"\"\"w = (X^T X + λI)^{-1} X^T y\"\"\"\n",
    "    def __init__(self, lam=1.0):\n",
    "        self.lam = lam\n",
    "    def fit(self, X, y):\n",
    "        d = X.shape[1]\n",
    "        A = X.T @ X + self.lam * torch.eye(d, device=X.device, dtype=X.dtype)\n",
    "        self.w = torch.linalg.solve(A, X.T @ y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return X @ self.w\n",
    "\n",
    "class RidgeDual:\n",
    "    \"\"\"α = (XX^T + λI)^{-1} y  — same answer, data appears only as dot products\"\"\"\n",
    "    def __init__(self, lam=1.0):\n",
    "        self.lam = lam\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        n = X.shape[0]\n",
    "        K = X @ X.T\n",
    "        self.alpha = torch.linalg.solve(K + self.lam * torch.eye(n, device=X.device, dtype=X.dtype), y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return (X @ self.X_train.T) @ self.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: primal and dual give the same predictions\n",
    "torch.manual_seed(0)\n",
    "X = torch.randn(50, 3, device=device, dtype=dtype)\n",
    "y = torch.randn(50, 1, device=device, dtype=dtype)\n",
    "X_new = torch.randn(10, 3, device=device, dtype=dtype)\n",
    "\n",
    "p1 = RidgePrimal(lam=1.0).fit(X, y).predict(X_new)\n",
    "p2 = RidgeDual(lam=1.0).fit(X, y).predict(X_new)\n",
    "print(f\"Max |primal - dual|: {(p1 - p2).abs().max():.2e}  (should be ~0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1",
   "metadata": {},
   "source": [
    "## Step 2: Kernel Ridge Regression\n",
    "\n",
    "In the dual form, data only appears through the Gram matrix $K_{ij} = x_i \\cdot x_j$.\n",
    "\n",
    "**The kernel trick:** replace the dot product with any kernel function $k(x_i, x_j)$:\n",
    "\n",
    "$$\\alpha = (K + \\lambda I)^{-1} y, \\qquad \\hat{y}_q = \\sum_i k(x_q, x_i)\\, \\alpha_i$$\n",
    "\n",
    "The algorithm is identical to dual ridge — just a different Gram matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1, X2, sigma=1.0):\n",
    "    \"\"\"k(x, x') = exp(-||x - x'||^2 / 2σ^2)\"\"\"\n",
    "    sq1 = (X1 ** 2).sum(dim=1, keepdim=True)\n",
    "    sq2 = (X2 ** 2).sum(dim=1, keepdim=True)\n",
    "    dist_sq = sq1 - 2.0 * X1 @ X2.T + sq2.T\n",
    "    return torch.exp(-dist_sq / (2.0 * sigma ** 2))\n",
    "\n",
    "class KernelRidge:\n",
    "    \"\"\"α = (K + λI)^{-1} y  with arbitrary kernel.\"\"\"\n",
    "    def __init__(self, kernel_fn=rbf_kernel, lam=1.0, **kw):\n",
    "        self.kernel_fn, self.lam, self.kw = kernel_fn, lam, kw\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        n = X.shape[0]\n",
    "        K = self.kernel_fn(X, X, **self.kw)\n",
    "        self.alpha = torch.linalg.solve(K + self.lam * torch.eye(n, device=X.device, dtype=X.dtype), y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.kernel_fn(X, self.X_train, **self.kw) @ self.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1",
   "metadata": {},
   "source": [
    "## Step 3: Kernel Regression = In-Context Learning\n",
    "\n",
    "Now the key connection. Consider a **single attention head** that receives context examples and a query:\n",
    "\n",
    "$$\\text{Attention}(x_q, \\{x_i, y_i\\}) = \\sum_i \\underbrace{\\frac{\\exp(x_q \\cdot x_i)}{\\sum_j \\exp(x_q \\cdot x_j)}}_{\\text{softmax attention weight}} \\cdot y_i$$\n",
    "\n",
    "This is the **Nadaraya-Watson kernel estimator** with the softmax kernel:\n",
    "\n",
    "$$\\hat{y}_q = \\frac{\\sum_i k(x_q, x_i)\\, y_i}{\\sum_j k(x_q, x_j)}$$\n",
    "\n",
    "The structure is identical:\n",
    "- **Keys** $= x_i$ (context inputs)\n",
    "- **Values** $= y_i$ (context labels)\n",
    "- **Query** $= x_q$ (the new input)\n",
    "- **Attention** = normalized kernel similarity\n",
    "- **Output** = similarity-weighted average of values\n",
    "\n",
    "**No weights are updated.** The prediction comes entirely from the context — just like kernel regression uses only the training set at test time.\n",
    "\n",
    "Below we implement both and compare them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nadaraya_watson(X_ctx, y_ctx, X_query, kernel_fn, **kw):\n",
    "    \"\"\"Nadaraya-Watson: ŷ = Σ k(xq, xi) yi / Σ k(xq, xj)\n",
    "    \n",
    "    This is what a single attention head computes.\n",
    "    \"\"\"\n",
    "    K = kernel_fn(X_query, X_ctx, **kw)   # (n_query, n_ctx) — like QK^T\n",
    "    weights = K / K.sum(dim=1, keepdim=True)  # normalize — like softmax\n",
    "    return weights @ y_ctx                    # weighted sum of values\n",
    "\n",
    "\n",
    "def attention_icl(X_ctx, y_ctx, X_query):\n",
    "    \"\"\"Single-head attention in-context learning.\n",
    "    \n",
    "    Keys = X_ctx, Values = y_ctx, Query = X_query.\n",
    "    Exactly the transformer ICL mechanism (one head, no projections).\n",
    "    \"\"\"\n",
    "    logits = X_query @ X_ctx.T                # (n_query, n_ctx) — QK^T\n",
    "    weights = torch.softmax(logits, dim=1)    # softmax attention\n",
    "    return weights @ y_ctx                    # weighted sum of values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1",
   "metadata": {},
   "source": [
    "## Demo: Noisy Sine Wave\n",
    "\n",
    "The \"context\" is $(x_i, y_i)$ pairs from a noisy $\\sin(x)$. We predict on new query points using:\n",
    "1. **Ridge** (primal) — fits a line, can't capture the curve\n",
    "2. **KRR** (RBF kernel) — kernel regression with ridge regularization\n",
    "3. **Nadaraya-Watson** (RBF) — kernel regression with normalization (= attention)\n",
    "4. **Attention ICL** — a literal single attention head over the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Context examples (= prompt examples in ICL)\n",
    "n_ctx = 200\n",
    "X_ctx = torch.linspace(-3, 3, n_ctx, device=device, dtype=dtype).unsqueeze(1)\n",
    "y_ctx = torch.sin(X_ctx) + 0.2 * torch.randn(n_ctx, 1, device=device, dtype=dtype)\n",
    "\n",
    "# Query points\n",
    "n_query = 100\n",
    "X_query = torch.linspace(-3, 3, n_query, device=device, dtype=dtype).unsqueeze(1)\n",
    "y_true  = torch.sin(X_query)\n",
    "\n",
    "# 1. Ridge (primal)\n",
    "pred_ridge = RidgePrimal(lam=1.0).fit(X_ctx, y_ctx).predict(X_query)\n",
    "\n",
    "# 2. Kernel Ridge Regression\n",
    "pred_krr = KernelRidge(kernel_fn=rbf_kernel, lam=0.01, sigma=0.5).fit(X_ctx, y_ctx).predict(X_query)\n",
    "\n",
    "# 3. Nadaraya-Watson (= attention with RBF kernel)\n",
    "pred_nw = nadaraya_watson(X_ctx, y_ctx, X_query, rbf_kernel, sigma=0.5)\n",
    "\n",
    "# 4. Attention ICL (softmax over raw dot products)\n",
    "pred_attn = attention_icl(X_ctx, y_ctx, X_query)\n",
    "\n",
    "mse = lambda a, b: ((a - b) ** 2).mean().item()\n",
    "print(\"MSE on query points:\")\n",
    "print(f\"  Ridge (linear):          {mse(y_true, pred_ridge):.6f}\")\n",
    "print(f\"  KRR (RBF):               {mse(y_true, pred_krr):.6f}\")\n",
    "print(f\"  Nadaraya-Watson (RBF):   {mse(y_true, pred_nw):.6f}\")\n",
    "print(f\"  Attention ICL (softmax): {mse(y_true, pred_attn):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4), sharex=True, sharey=True)\n",
    "\n",
    "xc = X_ctx.cpu().numpy().ravel()\n",
    "yc = y_ctx.cpu().numpy().ravel()\n",
    "xq = X_query.cpu().numpy().ravel()\n",
    "yt = y_true.cpu().numpy().ravel()\n",
    "\n",
    "titles = [\"Ridge (linear)\", \"KRR (RBF kernel)\", \"Nadaraya-Watson (RBF)\", \"Attention ICL (softmax)\"]\n",
    "preds  = [pred_ridge, pred_krr, pred_nw, pred_attn]\n",
    "\n",
    "for ax, title, pred in zip(axes, titles, preds):\n",
    "    ax.scatter(xc, yc, s=5, alpha=0.3, color=\"gray\", label=\"context\")\n",
    "    ax.plot(xq, yt, \"k--\", lw=1, label=\"true sin(x)\")\n",
    "    ax.plot(xq, pred.cpu().numpy().ravel(), \"r-\", lw=2, label=\"predicted\")\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "fig.suptitle(\"Kernel regression as in-context learning\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g1",
   "metadata": {},
   "source": [
    "## Visualizing the Attention / Kernel Weights\n",
    "\n",
    "For a single query point, we can visualize *which context examples contribute* to the prediction. This is the attention pattern — or equivalently, the kernel similarity profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a query point\n",
    "q_idx = 65  # somewhere on the right slope\n",
    "xq_single = X_query[q_idx:q_idx+1]\n",
    "\n",
    "# Kernel weights (Nadaraya-Watson)\n",
    "K_nw = rbf_kernel(xq_single, X_ctx, sigma=0.5)\n",
    "w_nw = (K_nw / K_nw.sum()).cpu().numpy().ravel()\n",
    "\n",
    "# Attention weights\n",
    "logits = xq_single @ X_ctx.T\n",
    "w_attn = torch.softmax(logits, dim=1).cpu().numpy().ravel()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3.5))\n",
    "\n",
    "for ax, w, title in [(ax1, w_nw, \"RBF kernel weights\"), (ax2, w_attn, \"Softmax attention weights\")]:\n",
    "    ax.bar(xc, w, width=0.04, color=\"steelblue\", alpha=0.7)\n",
    "    ax.axvline(xq_single.item(), color=\"red\", ls=\"--\", lw=2, label=f\"query = {xq_single.item():.2f}\")\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_xlabel(\"context x\")\n",
    "    ax.set_ylabel(\"weight\")\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(\"Which context examples matter for the prediction?\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h1",
   "metadata": {},
   "source": "## Takeaway So Far\n\nRaw attention ICL (softmax over raw dot products) is a **bad kernel** — it doesn't know what \"similar\" means for this problem.\n\nA real transformer doesn't use raw dot products. It learns projection matrices $W_Q, W_K, W_V$ during **pretraining** on many tasks. These projections define a **learned kernel**:\n\n$$k_\\theta(x_q, x_i) = \\text{softmax}\\!\\left(\\frac{(x_q W_Q)(x_i W_K)^\\top}{\\sqrt{d_k}}\\right)$$\n\nThis is the key: **pretraining = learning a good kernel**. Then at inference time, the learned kernel does ICL on new tasks it has never seen."
  },
  {
   "cell_type": "markdown",
   "id": "emndctp5aiv",
   "source": "## Step 4: Pre-trained Kernel for ICL\n\nWe'll train a single attention head on a **distribution of tasks** (random sinusoids with varying amplitude, frequency, phase). Each training step:\n\n1. Sample a random function $f$\n2. Generate context $(x_i, f(x_i) + \\text{noise})$ and query $(x_q, f(x_q))$\n3. Predict $\\hat{y}_q$ via attention with learned $W_Q, W_K, W_V$\n4. Backprop MSE loss\n\nAfter pretraining, we freeze the weights and test on a **new task never seen during training**. The learned kernel should do much better ICL than raw dot products.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kdn3t22afn",
   "source": "class PretrainedAttentionICL(torch.nn.Module):\n    \"\"\"Single attention head with learnable projections.\n    \n    Architecture mirrors kernel regression:\n      - Keys from x only    → the kernel measures similarity in input space\n      - Values from y only  → the output is a weighted sum of context labels\n      - Query from x only   → the query point\n    \n    This is a learned Nadaraya-Watson estimator.\n    \"\"\"\n    def __init__(self, input_dim=1, d_model=64):\n        super().__init__()\n        # Separate embeddings for x (used in Q, K) and y (used in V)\n        self.embed_x = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, d_model),\n            torch.nn.ReLU(),\n            torch.nn.Linear(d_model, d_model),\n        )\n        self.embed_y = torch.nn.Sequential(\n            torch.nn.Linear(1, d_model),\n            torch.nn.ReLU(),\n            torch.nn.Linear(d_model, d_model),\n        )\n        \n        # Q, K operate on x-embeddings → define the learned kernel\n        self.W_Q = torch.nn.Linear(d_model, d_model, bias=False)\n        self.W_K = torch.nn.Linear(d_model, d_model, bias=False)\n        # V operates on y-embeddings → read out labels\n        self.W_V = torch.nn.Linear(d_model, d_model, bias=False)\n        \n        self.out = torch.nn.Linear(d_model, 1)\n        self.scale = d_model ** 0.5\n    \n    def forward(self, X_ctx, y_ctx, X_query):\n        \"\"\"\n        X_ctx:   (batch, n_ctx, input_dim)\n        y_ctx:   (batch, n_ctx, 1)\n        X_query: (batch, n_query, input_dim)\n        Returns: (batch, n_query, 1)\n        \"\"\"\n        # Keys from context x, Values from context y, Queries from query x\n        K = self.W_K(self.embed_x(X_ctx))       # (batch, n_ctx, d_model)\n        V = self.W_V(self.embed_y(y_ctx))        # (batch, n_ctx, d_model)\n        Q = self.W_Q(self.embed_x(X_query))      # (batch, n_query, d_model)\n        \n        # Attention = learned kernel\n        weights = torch.softmax(Q @ K.transpose(-2, -1) / self.scale, dim=-1)\n        out = weights @ V                        # (batch, n_query, d_model)\n        \n        return self.out(out)                     # (batch, n_query, 1)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6nuaeqd37nv",
   "source": "### Task distribution for pretraining\n\nEach task is a random sinusoid: $f(x) = a \\sin(\\omega x + \\phi)$ with random amplitude $a$, frequency $\\omega$, and phase $\\phi$. The model never sees the same function twice — it must learn to **read the context** to figure out which function it's dealing with.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "j8glnmr1xq",
   "source": "def sample_task_batch(batch_size, n_ctx, n_query, noise_std=0.2):\n    \"\"\"Sample a batch of random sinusoid tasks.\n    \n    Each task:  f(x) = a * sin(ω*x + φ)  with random a, ω, φ.\n    Returns context (X_ctx, y_ctx) and query (X_query, y_query).\n    \"\"\"\n    # Random function parameters per task\n    a     = 0.5 + 1.5 * torch.rand(batch_size, 1, 1, device=device)    # amplitude [0.5, 2]\n    omega = 0.5 + 2.0 * torch.rand(batch_size, 1, 1, device=device)    # frequency [0.5, 2.5]\n    phi   = 2 * 3.14159 * torch.rand(batch_size, 1, 1, device=device)  # phase [0, 2π]\n    \n    # Sample x locations uniformly\n    X_all = 6.0 * torch.rand(batch_size, n_ctx + n_query, 1, device=device) - 3.0  # [-3, 3]\n    y_all = a * torch.sin(omega * X_all + phi)\n    \n    # Split into context and query\n    X_ctx   = X_all[:, :n_ctx]\n    y_ctx   = y_all[:, :n_ctx] + noise_std * torch.randn_like(y_all[:, :n_ctx])\n    X_query = X_all[:, n_ctx:]\n    y_query = y_all[:, n_ctx:]  # clean targets for query\n    \n    return X_ctx, y_ctx, X_query, y_query",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "o1m7n744ewj",
   "source": "### Pretraining loop\n\nTrain the attention head on many random tasks. This is analogous to pretraining a transformer on diverse data — the model learns a kernel that works across tasks.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yd0u50xrsc",
   "source": "torch.manual_seed(42)\n\nmodel = PretrainedAttentionICL(input_dim=1, d_model=64).to(device).float()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nn_steps = 5000\nbatch_size = 64\nn_ctx_train = 40\nn_query_train = 10\n\nlosses = []\nfor step in range(n_steps):\n    X_c, y_c, X_q, y_q = sample_task_batch(batch_size, n_ctx_train, n_query_train)\n    \n    pred = model(X_c.float(), y_c.float(), X_q.float())\n    loss = ((pred - y_q.float()) ** 2).mean()\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    \n    if step == 0 or (step + 1) % 1000 == 0:\n        print(f\"  Step {step+1:5d}  loss: {loss.item():.4f}\")\n\nplt.figure(figsize=(8, 3))\nplt.plot(losses, alpha=0.3, color=\"steelblue\")\nwindow = 100\nsmoothed = [sum(losses[max(0,i-window):i+1])/len(losses[max(0,i-window):i+1]) for i in range(len(losses))]\nplt.plot(smoothed, color=\"darkblue\", lw=2)\nplt.xlabel(\"Training step\")\nplt.ylabel(\"MSE loss\")\nplt.title(\"Pretraining: learning a kernel across many tasks\")\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "o1gjong8p1m",
   "source": "### Evaluation on a new task\n\nNow we freeze the model and test on **sin(x)** — the same task from earlier. The model has never seen this specific function during pretraining, but it has learned a kernel that can read any sinusoidal context.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "w2xsvdqvw5d",
   "source": "# Same test data as before\ntorch.manual_seed(42)\nn_ctx = 200\nX_ctx = torch.linspace(-3, 3, n_ctx, device=device, dtype=dtype).unsqueeze(1)\ny_ctx = torch.sin(X_ctx) + 0.2 * torch.randn(n_ctx, 1, device=device, dtype=dtype)\nn_query = 100\nX_query = torch.linspace(-3, 3, n_query, device=device, dtype=dtype).unsqueeze(1)\ny_true  = torch.sin(X_query)\n\n# Run pre-trained model (no weight updates — pure ICL)\nmodel.eval()\nwith torch.no_grad():\n    pred_pretrained = model(\n        X_ctx.unsqueeze(0).float(),\n        y_ctx.unsqueeze(0).float(),\n        X_query.unsqueeze(0).float()\n    ).squeeze(0).double()\n\n# Compare all methods\npred_ridge = RidgePrimal(lam=1.0).fit(X_ctx, y_ctx).predict(X_query)\npred_krr   = KernelRidge(kernel_fn=rbf_kernel, lam=0.01, sigma=0.5).fit(X_ctx, y_ctx).predict(X_query)\npred_nw    = nadaraya_watson(X_ctx, y_ctx, X_query, rbf_kernel, sigma=0.5)\npred_attn  = attention_icl(X_ctx, y_ctx, X_query)\n\nmse = lambda a, b: ((a - b) ** 2).mean().item()\nprint(\"MSE on query points:\")\nprint(f\"  Ridge (linear):               {mse(y_true, pred_ridge):.6f}\")\nprint(f\"  Attention ICL (raw, no train): {mse(y_true, pred_attn):.6f}\")\nprint(f\"  Nadaraya-Watson (RBF):         {mse(y_true, pred_nw):.6f}\")\nprint(f\"  KRR (RBF):                     {mse(y_true, pred_krr):.6f}\")\nprint(f\"  Attention ICL (pre-trained):   {mse(y_true, pred_pretrained):.6f}  ← learned kernel\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "grnsdhfdoc6",
   "source": "fig, axes = plt.subplots(1, 5, figsize=(22, 4), sharex=True, sharey=True)\n\nxc = X_ctx.cpu().numpy().ravel()\nyc = y_ctx.cpu().numpy().ravel()\nxq = X_query.cpu().numpy().ravel()\nyt = y_true.cpu().numpy().ravel()\n\ntitles = [\"Ridge (linear)\", \"Attention ICL\\n(raw)\", \"Nadaraya-Watson\\n(RBF)\", \"KRR (RBF)\", \"Attention ICL\\n(pre-trained)\"]\npreds  = [pred_ridge, pred_attn, pred_nw, pred_krr, pred_pretrained]\n\nfor ax, title, pred in zip(axes, titles, preds):\n    ax.scatter(xc, yc, s=5, alpha=0.3, color=\"gray\", label=\"context\")\n    ax.plot(xq, yt, \"k--\", lw=1, label=\"true sin(x)\")\n    ax.plot(xq, pred.cpu().numpy().ravel(), \"r-\", lw=2, label=\"predicted\")\n    ax.set_title(title, fontsize=10)\n    ax.legend(fontsize=7)\n\nfig.suptitle(\"Pre-trained kernel learns to do in-context learning\", fontsize=14)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jorwh8vmxne",
   "source": "## Full Summary\n\n### 1-D results (sin(x))\n\n| Method | Kernel | MSE |\n|---|---|---|\n| Ridge | Linear | 0.1713 |\n| Attention ICL (raw) | Softmax dot product | 0.0806 |\n| Nadaraya-Watson | RBF | 0.0147 |\n| GA ICL (penalty) | $\\beta_1\\cos\\theta - \\beta_2\\sin\\theta$ | 0.0082 |\n| Cayley ICL | $-\\theta^2/(2T^2)$ | 0.0074 |\n| KRR (RBF) | RBF | 0.0042 |\n| Pre-trained attention | Learned softmax | 0.0039 |\n| Cosine ICL | $\\cos(\\theta)/T$ | 0.0038 |\n\n### High-D results ($x \\in \\mathbb{R}^{10}$ with correlated feature groups)\n\nTask: 3 of 10 features share a hidden latent $z$; target $y = a\\sin(bz + \\phi)$. The model must discover which features belong to the group from context alone. Evaluated on 200 held-out tasks.\n\n| Row Kernel | Row-only | Col+Row | Improvement |\n|---|---|---|---|\n| Softmax | 0.0997 | 0.0780 | +21.8% |\n| Cosine | 0.0864 | 0.0718 | +16.9% |\n| Cayley | 0.0861 | 0.0718 | +16.6% |\n| GA | 0.0862 | 0.0720 | +16.4% |\n| *KRR (RBF)* | *0.1165* | — | — |\n\n### Key insights\n\n1. **Ridge → Dual → Kernel** — the algebraic path from familiar to powerful\n2. **Kernel regression = ICL** — attention over context IS kernel regression\n3. **The kernel matters** — Cosine, Cayley ($\\theta$), and GA kernels each bring different geometric structure. In 1-D, Cosine ICL (0.0038) matches the learned kernel and beats hand-tuned KRR (0.0042)\n4. **Pretraining = learning the kernel** — learned $W_Q, W_K, W_V$ adapt to the task distribution\n5. **GA row kernels outperform softmax** — in higher dimensions, all three geometric kernels (Cosine 0.086, Cayley 0.086, GA 0.086) beat softmax (0.100) for row-only ICL\n6. **Column attention helps consistently** — GA column attention improves every row kernel by 16–22%, discovering correlated feature groups from context data\n7. **Column attention + GA kernels = best overall** — Col+Cosine and Col+Cayley (0.072) beat all other methods including KRR (0.117). The combination of geometric column attention (feature groups) and geometric row attention (sample similarity) gives the strongest ICL",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "29ub7eipt25",
   "source": "## Step 5: Geometric Algebra Kernels — cos($\\theta$) and $\\theta$ (Cayley)\n\nStandard attention uses the raw dot product as its kernel. But we can design **geometrically motivated kernels** that measure similarity differently.\n\nFrom `ga_transformer`, we bring two attention mechanisms that operate on **normalized** vectors and use the **rotation angle** $\\theta$ between them:\n\n| Kernel | Score | Interpretation |\n|---|---|---|\n| **Cosine** | $\\cos(\\theta) / T$ | Standard cosine similarity — alignment |\n| **Cayley ($\\theta$)** | $-\\theta^2 / (2T^2)$ | Squared rotation angle — like an RBF in angle space |\n| **GA (inner + wedge)** | $\\beta_1 \\cos\\theta + f(\\sin\\theta)$ | Alignment + orthogonality structure |\n\nThe Cayley kernel is particularly interesting: $\\text{score} = -\\theta^2/(2T^2)$ is a **Gaussian kernel in angle space** — nearby directions get high weight, distant directions get low weight, just like RBF but on the hypersphere.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hpb6eqe5e2",
   "source": "import sys\nsys.path.insert(0, \"/home/asudjianto/jupyterlab/ga_transformer\")\nfrom ga_icl.attention import CayleyAttention, GAAttention, CosineAttention\n\n# --- GA-based ICL predictors (Nadaraya-Watson with GA kernels) ---\n# These take raw X, normalize, compute attention weights, then ŷ = weights @ y\n\ndef cosine_icl(X_ctx, y_ctx, X_query, temperature=0.1):\n    \"\"\"ICL using cosine similarity kernel: score = cos(θ) / T\"\"\"\n    attn = CosineAttention(temperature=temperature)\n    z_ctx   = X_ctx / X_ctx.norm(dim=1, keepdim=True).clamp(min=1e-8)\n    z_query = X_query / X_query.norm(dim=1, keepdim=True).clamp(min=1e-8)\n    weights = attn(z_query, z_ctx)\n    return weights @ y_ctx\n\ndef cayley_icl(X_ctx, y_ctx, X_query, temperature=0.1):\n    \"\"\"ICL using Cayley (θ) kernel: score = -θ² / (2T²)\n    \n    θ = arccos(cos(θ)) = rotation angle between normalized vectors.\n    This is a Gaussian kernel in angle space.\n    \"\"\"\n    attn = CayleyAttention(temperature=temperature)\n    z_ctx   = X_ctx / X_ctx.norm(dim=1, keepdim=True).clamp(min=1e-8)\n    z_query = X_query / X_query.norm(dim=1, keepdim=True).clamp(min=1e-8)\n    weights = attn(z_query, z_ctx)\n    return weights @ y_ctx\n\ndef ga_icl(X_ctx, y_ctx, X_query, beta1=4.0, beta2=1.0, temperature=1.0,\n           wedge_mode=\"penalty\"):\n    \"\"\"ICL using GA kernel: score = β₁·cos(θ) + f(sin(θ))\"\"\"\n    attn = GAAttention(beta1=beta1, beta2=beta2, temperature=temperature,\n                       wedge_mode=wedge_mode)\n    z_ctx   = X_ctx / X_ctx.norm(dim=1, keepdim=True).clamp(min=1e-8)\n    z_query = X_query / X_query.norm(dim=1, keepdim=True).clamp(min=1e-8)\n    weights = attn(z_query, z_ctx)\n    return weights @ y_ctx\n\nprint(\"GA attention modules loaded.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "oa3aj2ohpim",
   "source": "### Comparing GA kernels on the sin(x) task\n\nSame context, same queries — we just swap the kernel that computes the attention weights.\n\n**Important:** These kernels operate on **normalized** vectors. For 1-D input, $x / \\|x\\|$ collapses to $\\pm 1$ (sign only). So we need to embed our scalar $x$ into a higher-dimensional space first — we'll use a simple random Fourier feature embedding to give the kernels something meaningful to work with.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "q8hvbklzy9i",
   "source": "# Embed scalar x into higher-D space so normalization is meaningful.\n# Random Fourier Features: φ(x) = [cos(w₁x), sin(w₁x), cos(w₂x), sin(w₂x), ...]\n# This is a standard trick for making RBF-like features explicit.\n\ntorch.manual_seed(42)\nn_rff = 32  # number of random frequencies\nW_rff = torch.randn(1, n_rff, device=device, dtype=dtype) * 2.0  # random frequencies\n\ndef rff_embed(X):\n    \"\"\"Random Fourier Features: (n, 1) → (n, 2*n_rff)\"\"\"\n    proj = X @ W_rff  # (n, n_rff)\n    return torch.cat([torch.cos(proj), torch.sin(proj)], dim=1)\n\n# Re-use same test data\ntorch.manual_seed(42)\nn_ctx = 200\nX_ctx = torch.linspace(-3, 3, n_ctx, device=device, dtype=dtype).unsqueeze(1)\ny_ctx = torch.sin(X_ctx) + 0.2 * torch.randn(n_ctx, 1, device=device, dtype=dtype)\nn_query = 100\nX_query = torch.linspace(-3, 3, n_query, device=device, dtype=dtype).unsqueeze(1)\ny_true  = torch.sin(X_query)\n\n# Embed into RFF space\nZ_ctx   = rff_embed(X_ctx)\nZ_query = rff_embed(X_query)\n\n# --- Run all GA kernels ---\npred_cosine = cosine_icl(Z_ctx, y_ctx, Z_query, temperature=0.1)\npred_cayley = cayley_icl(Z_ctx, y_ctx, Z_query, temperature=0.1)\npred_ga_pen = ga_icl(Z_ctx, y_ctx, Z_query, beta1=4.0, beta2=1.0,\n                     temperature=1.0, wedge_mode=\"penalty\")\n\n# Baselines for comparison\npred_krr  = KernelRidge(kernel_fn=rbf_kernel, lam=0.01, sigma=0.5).fit(X_ctx, y_ctx).predict(X_query)\npred_nw   = nadaraya_watson(X_ctx, y_ctx, X_query, rbf_kernel, sigma=0.5)\n\nmse = lambda a, b: ((a - b) ** 2).mean().item()\nprint(\"MSE on query points:\")\nprint(f\"  KRR (RBF):              {mse(y_true, pred_krr):.6f}\")\nprint(f\"  Nadaraya-Watson (RBF):  {mse(y_true, pred_nw):.6f}\")\nprint(f\"  Cosine ICL (cos θ/T):   {mse(y_true, pred_cosine):.6f}\")\nprint(f\"  Cayley ICL (-θ²/2T²):   {mse(y_true, pred_cayley):.6f}\")\nprint(f\"  GA ICL (penalty):       {mse(y_true, pred_ga_pen):.6f}\")\nprint(f\"  Pre-trained attention:  {mse(y_true, pred_pretrained):.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "lnr8wbbv0ze",
   "source": "fig, axes = plt.subplots(2, 3, figsize=(16, 7), sharex=True, sharey=True)\n\nxc = X_ctx.cpu().numpy().ravel()\nyc = y_ctx.cpu().numpy().ravel()\nxq = X_query.cpu().numpy().ravel()\nyt = y_true.cpu().numpy().ravel()\n\nall_titles = [\n    \"KRR (RBF)\", \"Nadaraya-Watson (RBF)\", \"Pre-trained attention\",\n    r\"Cosine ICL ($\\cos\\theta / T$)\", r\"Cayley ICL ($-\\theta^2/2T^2$)\",\n    r\"GA ICL ($\\beta_1\\cos\\theta - \\beta_2\\sin\\theta$)\",\n]\nall_preds = [pred_krr, pred_nw, pred_pretrained,\n             pred_cosine, pred_cayley, pred_ga_pen]\n\nfor ax, title, pred in zip(axes.ravel(), all_titles, all_preds):\n    ax.scatter(xc, yc, s=5, alpha=0.3, color=\"gray\", label=\"context\")\n    ax.plot(xq, yt, \"k--\", lw=1, label=\"true sin(x)\")\n    ax.plot(xq, pred.cpu().detach().numpy().ravel(), \"r-\", lw=2, label=\"predicted\")\n    cur_mse = mse(y_true, pred)\n    ax.set_title(f\"{title}\\nMSE = {cur_mse:.4f}\", fontsize=10)\n    ax.legend(fontsize=7)\n\nfig.suptitle(\"GA kernels as in-context learning\", fontsize=14)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "46ferjvjcgi",
   "source": "### Kernel weight profiles: RBF vs Cosine vs Cayley ($\\theta$)\n\nHow do these kernels distribute attention across context points for a single query?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rjz81676jjd",
   "source": "# Weight profiles for a single query point\nq_idx = 65\nzq = Z_query[q_idx:q_idx+1]\nzc = Z_ctx\n\n# Normalize for GA kernels\neps = 1e-8\nzq_n = zq / zq.norm(dim=1, keepdim=True).clamp(min=eps)\nzc_n = zc / zc.norm(dim=1, keepdim=True).clamp(min=eps)\n\n# RBF weights (on raw X)\nxq_s = X_query[q_idx:q_idx+1]\nK_rbf = rbf_kernel(xq_s, X_ctx, sigma=0.5)\nw_rbf = (K_rbf / K_rbf.sum()).cpu().numpy().ravel()\n\n# Cosine weights\nw_cos = CosineAttention(temperature=0.1)(zq_n, zc_n).detach().cpu().numpy().ravel()\n\n# Cayley weights\nw_cay = CayleyAttention(temperature=0.1)(zq_n, zc_n).detach().cpu().numpy().ravel()\n\n# GA penalty weights\nw_ga = GAAttention(beta1=4.0, beta2=1.0, temperature=1.0,\n                   wedge_mode=\"penalty\")(zq_n, zc_n).detach().cpu().numpy().ravel()\n\nfig, axes = plt.subplots(1, 4, figsize=(18, 3.5), sharex=True)\n\nquery_val = X_query[q_idx].item()\nfor ax, w, title in zip(axes,\n        [w_rbf, w_cos, w_cay, w_ga],\n        [\"RBF kernel\", r\"Cosine ($\\cos\\theta/T$)\",\n         r\"Cayley ($-\\theta^2/2T^2$)\", r\"GA ($\\beta_1\\cos\\theta - \\beta_2\\sin\\theta$)\"]):\n    ax.bar(xc, w, width=0.04, color=\"steelblue\", alpha=0.7)\n    ax.axvline(query_val, color=\"red\", ls=\"--\", lw=2, label=f\"query = {query_val:.2f}\")\n    ax.set_title(title, fontsize=10)\n    ax.set_xlabel(\"context x\")\n    ax.legend(fontsize=8)\n\naxes[0].set_ylabel(\"weight\")\nfig.suptitle(\"Attention weight profiles for a single query point\", fontsize=13)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "i44vrqerus",
   "source": "## Step 6: Higher Dimensions — Column Attention for Feature Interactions\n\nEverything so far used 1-D input. In higher dimensions ($x \\in \\mathbb{R}^d$), features can form **correlated groups** — and the task may depend on these hidden groups rather than on individual features.\n\nStandard attention computes **row attention** (query vs context). But GA introduces **column attention** (feature vs feature):\n\n| | Row attention | Column attention |\n|---|---|---|\n| **Compares** | Samples to samples | Features to features |\n| **Matrix** | $(n \\times n)$ | $(d \\times d)$ |\n| **Purpose** | Which context examples matter? | Which features belong together? |\n| **GA insight** | $\\cos\\theta$ = sample similarity | $\\sin\\theta$ (wedge) = feature independence |\n\nThe pipeline:\n1. **Column attention** discovers which features are correlated and mixes them — denoising the group signal\n2. **Row attention** uses the cleaner representations to find relevant context examples\n3. Prediction = weighted sum of context labels\n\nOur task distribution creates **random feature groups**: a random subset of features shares a latent source, and the target depends on this hidden group. Column attention can discover the group structure (correlated features have similar activation patterns across the $n$ context samples) and denoise by averaging within groups.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4wo1finieey",
   "source": "from ga_icl.attention import GAColumnAttention\n\n# --- High-D task: random correlated feature groups ---\n# Column attention should discover which features are correlated and mix them.\n\ninput_dim = 10   # more features → more room for column attention to help\n\ndef sample_hd_task_batch(batch_size, n_ctx, n_query, input_dim=10, noise_std=0.1):\n    \"\"\"Sample tasks with random correlated feature groups.\n    \n    Each task:\n      1. Pick a random subset of k features to be \"group\" features\n      2. These k features all share a latent signal z (plus individual noise)\n      3. Target = a * sin(b * z + phi) — depends on the latent, not individual features\n      4. Non-group features are pure noise (distractors)\n    \n    Column attention can discover the group (correlated features have similar\n    activation patterns) and average them to denoise z.\n    Without column attention, the model must implicitly figure out which\n    features matter from the context — much harder.\n    \"\"\"\n    k = 3  # group size (k of input_dim features share a latent)\n    \n    # Random function parameters per task\n    a   = 0.5 + 1.5 * torch.rand(batch_size, 1, 1, device=device)\n    b   = 0.5 + 2.0 * torch.rand(batch_size, 1, 1, device=device)\n    phi = 2 * 3.14159 * torch.rand(batch_size, 1, 1, device=device)\n    \n    n_total = n_ctx + n_query\n    \n    # Latent signal shared by group features\n    z = torch.randn(batch_size, n_total, 1, device=device)\n    \n    # Build X: group features = z + noise, distractor features = pure noise\n    X_all = torch.randn(batch_size, n_total, input_dim, device=device) * 0.5  # all noise baseline\n    \n    # For each batch, pick k random features to be the group\n    for bi in range(batch_size):\n        group_idx = torch.randperm(input_dim, device=device)[:k]\n        for fi in group_idx:\n            X_all[bi, :, fi] = z[bi, :, 0] + 0.3 * torch.randn(n_total, device=device)\n    \n    # Target depends on latent signal\n    y_all = a * torch.sin(b * z + phi)\n    \n    X_ctx   = X_all[:, :n_ctx]\n    y_ctx   = y_all[:, :n_ctx] + noise_std * torch.randn_like(y_all[:, :n_ctx])\n    X_query = X_all[:, n_ctx:]\n    y_query = y_all[:, n_ctx:]\n    return X_ctx, y_ctx, X_query, y_query\n\nX_c, y_c, X_q, y_q = sample_hd_task_batch(4, 100, 20, input_dim=input_dim)\nprint(f\"Task: 3 of {input_dim} features share a latent → y = a·sin(b·z + φ)\")\nprint(f\"Context: X {tuple(X_c.shape)}, y {tuple(y_c.shape)}\")\nprint(f\"Query:   X {tuple(X_q.shape)}, y {tuple(y_q.shape)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4ruwotugbwi",
   "source": "### Unified ICL model: Column attention × Row kernel\n\nWe systematically compare all combinations of:\n- **Column attention**: off (row-only) vs on (column+row)\n- **Row kernel**: Softmax, Cosine ($\\cos\\theta/T$), Cayley ($-\\theta^2/2T^2$), GA ($\\beta_1\\cos\\theta - \\beta_2\\sin\\theta$)\n\nThe GA kernel parameters in row attention are **learnable** — they adapt during training just like the column attention parameters. This gives the geometric kernels a fair chance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "87xtuf8hvsf",
   "source": "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass LearnableColumnAttention(nn.Module):\n    \"\"\"GA Column Attention with LEARNABLE parameters + residual gate.\"\"\"\n    def __init__(self, d_model, init_beta1=4.0, init_beta2=1.0,\n                 init_diag_bias=2.0, init_wedge_mu=0.5):\n        super().__init__()\n        self.beta1 = nn.Parameter(torch.tensor(init_beta1))\n        self.beta2 = nn.Parameter(torch.tensor(init_beta2))\n        self.diag_bias = nn.Parameter(torch.tensor(init_diag_bias))\n        self.wedge_mu = nn.Parameter(torch.tensor(init_wedge_mu))\n        self.gate = nn.Parameter(torch.tensor(0.0))\n    \n    def forward(self, X):\n        B, N, D = X.shape\n        eps = 1e-6\n        X_t = X.transpose(1, 2)\n        u = X_t / X_t.norm(dim=-1, keepdim=True).clamp(min=eps)\n        c = torch.einsum(\"bdn,ben->bde\", u, u).clamp(-1 + eps, 1 - eps)\n        w = torch.sqrt((1 - c ** 2).clamp(min=0))\n        score = (self.beta1 * c\n                 - self.beta2 * (w - self.wedge_mu) ** 2\n                 + self.diag_bias * torch.eye(D, device=X.device).unsqueeze(0))\n        alpha = F.softmax(score, dim=-1)\n        Y = torch.einsum(\"bde,ben->bdn\", alpha, X_t).transpose(1, 2)\n        g = torch.sigmoid(self.gate)\n        return (1 - g) * X + g * Y\n\n\nclass UnifiedICL(nn.Module):\n    \"\"\"Unified ICL model: {column attention} × {row kernel type}.\n    \n    Args:\n        input_dim: Raw input dimension\n        d_model: Embedding dimension\n        use_column_attn: Whether to apply GA column attention on raw features\n        row_kernel: \"softmax\", \"cosine\", \"cayley\", or \"ga\"\n    \"\"\"\n    def __init__(self, input_dim=10, d_model=64, use_column_attn=False,\n                 row_kernel=\"softmax\"):\n        super().__init__()\n        self.use_column_attn = use_column_attn\n        self.row_kernel = row_kernel\n        \n        if use_column_attn:\n            self.col_attn = LearnableColumnAttention(d_model=input_dim)\n        \n        self.embed_x = nn.Sequential(\n            nn.Linear(input_dim, d_model), nn.ReLU(), nn.Linear(d_model, d_model))\n        self.embed_y = nn.Sequential(\n            nn.Linear(1, d_model), nn.ReLU(), nn.Linear(d_model, d_model))\n        \n        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n        self.W_K = nn.Linear(d_model, d_model, bias=False)\n        self.W_V = nn.Linear(d_model, d_model, bias=False)\n        self.out = nn.Linear(d_model, 1)\n        self.scale = d_model ** 0.5\n        \n        # Learnable row-kernel parameters\n        if row_kernel in (\"cosine\", \"cayley\"):\n            self.temperature = nn.Parameter(torch.tensor(0.1))\n        if row_kernel == \"ga\":\n            self.temperature = nn.Parameter(torch.tensor(1.0))\n            self.ga_beta1 = nn.Parameter(torch.tensor(4.0))\n            self.ga_beta2 = nn.Parameter(torch.tensor(1.0))\n    \n    def _row_weights(self, Q, K):\n        \"\"\"Compute row attention weights using the chosen kernel.\"\"\"\n        if self.row_kernel == \"softmax\":\n            return F.softmax(Q @ K.transpose(-2, -1) / self.scale, dim=-1)\n        \n        # Normalize for angle-based kernels\n        eps = 1e-6\n        Q_n = Q / Q.norm(dim=-1, keepdim=True).clamp(min=eps)\n        K_n = K / K.norm(dim=-1, keepdim=True).clamp(min=eps)\n        cos_theta = (Q_n @ K_n.transpose(-2, -1)).clamp(-1 + eps, 1 - eps)\n        \n        if self.row_kernel == \"cosine\":\n            score = cos_theta / self.temperature\n        elif self.row_kernel == \"cayley\":\n            theta = torch.acos(cos_theta)\n            score = -theta ** 2 / (2 * self.temperature ** 2)\n        elif self.row_kernel == \"ga\":\n            sin_theta = torch.sqrt((1 - cos_theta ** 2).clamp(min=0))\n            score = (self.ga_beta1 * cos_theta - self.ga_beta2 * sin_theta) / self.temperature\n        \n        return F.softmax(score, dim=-1)\n    \n    def forward(self, X_ctx, y_ctx, X_query):\n        # Optional column attention on raw features\n        if self.use_column_attn:\n            X_ctx = self.col_attn(X_ctx)\n            X_query = self.col_attn(X_query)\n        \n        # Embed\n        ctx_x = self.embed_x(X_ctx)\n        qry_x = self.embed_x(X_query)\n        ctx_y = self.embed_y(y_ctx)\n        \n        # Row attention with chosen kernel\n        Q = self.W_Q(qry_x)\n        K = self.W_K(ctx_x)\n        V = self.W_V(ctx_y)\n        weights = self._row_weights(Q, K)\n        return self.out(weights @ V)\n\n# Verify it works\nm = UnifiedICL(input_dim=10, use_column_attn=True, row_kernel=\"ga\").to(device).float()\nX_c, y_c, X_q, y_q = sample_hd_task_batch(2, 50, 10, input_dim=10)\nprint(f\"UnifiedICL output shape: {m(X_c, y_c, X_q).shape}\")\nprint(f\"Row kernel types: softmax, cosine, cayley, ga\")\nprint(f\"Column attention: on/off\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "484q4fmz5ft",
   "source": "### Training all 8 combinations\n\nWe train every combination of `{Row-only, Column+Row} × {Softmax, Cosine, Cayley, GA}` on the same task distribution. This gives a clean 2×4 comparison.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "b0rcp5osjlq",
   "source": "def train_model(model, n_steps=10000, batch_size=64, n_ctx=100, n_query=20,\n                input_dim=10, lr=1e-3):\n    \"\"\"Train a model on random high-dimensional tasks.\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    losses = []\n    for step in range(n_steps):\n        X_c, y_c, X_q, y_q = sample_hd_task_batch(\n            batch_size, n_ctx, n_query, input_dim=input_dim)\n        pred = model(X_c.float(), y_c.float(), X_q.float())\n        loss = ((pred - y_q.float()) ** 2).mean()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n    return losses\n\ntorch.manual_seed(42)\n\nrow_kernels = [\"softmax\", \"cosine\", \"cayley\", \"ga\"]\nmodels = {}      # (col_attn, kernel) → model\nall_losses = {}  # (col_attn, kernel) → losses\n\nfor use_col in [False, True]:\n    for kernel in row_kernels:\n        tag = (\"Col+\" if use_col else \"\") + kernel.capitalize()\n        print(f\"Training {tag}...\", end=\" \", flush=True)\n        m = UnifiedICL(input_dim=input_dim, d_model=64,\n                       use_column_attn=use_col, row_kernel=kernel).to(device).float()\n        losses = train_model(m, n_steps=10000, input_dim=input_dim)\n        models[(use_col, kernel)] = m\n        all_losses[(use_col, kernel)] = losses\n        print(f\"final loss: {losses[-1]:.4f}\")\n\n# Show learned column attention parameters for Col+Softmax\nca = models[(True, \"softmax\")].col_attn\nprint(f\"\\nLearned column attention params (Col+Softmax):\")\nprint(f\"  beta1={ca.beta1.item():.2f}  beta2={ca.beta2.item():.2f}  \"\n      f\"diag_bias={ca.diag_bias.item():.2f}  wedge_mu={ca.wedge_mu.item():.2f}  \"\n      f\"gate={torch.sigmoid(ca.gate).item():.2f}\")\n\n# Training curves\nfig, axes = plt.subplots(1, 4, figsize=(18, 3.5), sharey=True)\nwindow = 200\ncolors_row = {\"softmax\": \"coral\", \"cosine\": \"#e67e22\", \"cayley\": \"#e74c3c\", \"ga\": \"#c0392b\"}\ncolors_col = {\"softmax\": \"steelblue\", \"cosine\": \"#2980b9\", \"cayley\": \"#3498db\", \"ga\": \"#1abc9c\"}\n\nfor ax, kernel in zip(axes, row_kernels):\n    for use_col, cmap in [(False, colors_row), (True, colors_col)]:\n        losses = all_losses[(use_col, kernel)]\n        sm = [sum(losses[max(0,i-window):i+1])/len(losses[max(0,i-window):i+1])\n              for i in range(len(losses))]\n        label = (\"Col+\" if use_col else \"Row \") + kernel\n        ax.plot(losses, alpha=0.08, color=cmap[kernel])\n        ax.plot(sm, color=cmap[kernel], lw=2, label=label)\n    ax.set_title(kernel.capitalize(), fontsize=12)\n    ax.set_xlabel(\"Step\")\n    ax.legend(fontsize=8)\naxes[0].set_ylabel(\"MSE loss\")\nfig.suptitle(f\"Training curves: Row-only vs Column+Row × kernel type (dim={input_dim})\", fontsize=13)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mjx2yang9nb",
   "source": "### Evaluation: 2×4 comparison on held-out tasks\n\nTest all 8 trained models + KRR baseline on 200 new tasks. This reveals:\n- Which **row kernel** works best for ICL?\n- Does **column attention** help consistently across kernel types?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "a1tmq3yzblv",
   "source": "torch.manual_seed(123)\n\nn_eval_tasks = 200\nn_ctx_eval = 200\nn_query_eval = 50\n\n# Result containers\nresult_names = [\"KRR (RBF)\"]\nfor use_col in [False, True]:\n    for kernel in row_kernels:\n        tag = (\"Col+\" if use_col else \"Row \") + kernel.capitalize()\n        result_names.append(tag)\nresults = {name: [] for name in result_names}\n\nfor m in models.values():\n    m.eval()\n\nfor t in range(n_eval_tasks):\n    X_c, y_c, X_q, y_q = sample_hd_task_batch(\n        1, n_ctx_eval, n_query_eval, input_dim=input_dim, noise_std=0.1)\n    \n    xc = X_c.squeeze(0).double()\n    yc = y_c.squeeze(0).double()\n    xq = X_q.squeeze(0).double()\n    yq = y_q.squeeze(0).double()\n    \n    mse_fn = lambda pred: ((pred - yq) ** 2).mean().item()\n    \n    # KRR baseline\n    results[\"KRR (RBF)\"].append(\n        mse_fn(KernelRidge(kernel_fn=rbf_kernel, lam=0.01, sigma=3.0).fit(xc, yc).predict(xq)))\n    \n    # All 8 trained models\n    with torch.no_grad():\n        for use_col in [False, True]:\n            for kernel in row_kernels:\n                tag = (\"Col+\" if use_col else \"Row \") + kernel.capitalize()\n                m = models[(use_col, kernel)]\n                pred = m(X_c.float(), y_c.float(), X_q.float()).squeeze(0).double()\n                results[tag].append(mse_fn(pred))\n\n# Print results table\nprint(f\"Mean MSE over {n_eval_tasks} held-out tasks (dim = {input_dim}):\\n\")\nprint(f\"  {'Method':20s}  {'MSE':>8s}\")\nprint(f\"  {'─'*20}  {'─'*8}\")\nfor name in result_names:\n    mean_mse = sum(results[name]) / len(results[name])\n    print(f\"  {name:20s}  {mean_mse:.4f}\")\n\n# Column attention improvement per kernel\nprint(f\"\\nColumn attention improvement per kernel:\")\nfor kernel in row_kernels:\n    row_tag = \"Row \" + kernel.capitalize()\n    col_tag = \"Col+\" + kernel.capitalize()\n    row_mse = sum(results[row_tag]) / len(results[row_tag])\n    col_mse = sum(results[col_tag]) / len(results[col_tag])\n    pct = 100 * (row_mse - col_mse) / row_mse\n    print(f\"  {kernel:8s}:  Row {row_mse:.4f} → Col+Row {col_mse:.4f}  ({pct:+.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "h3qdyg8543l",
   "source": "# Grouped bar chart: Row-only vs Column+Row for each kernel\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Box plot of all methods\nnames = result_names\ndata = [results[n] for n in names]\nbp = ax1.boxplot(data, labels=[n.replace(\"Row \", \"R:\").replace(\"Col+\", \"C+\") for n in names],\n                 patch_artist=True, showfliers=False, medianprops=dict(color=\"black\", lw=2))\ncolors = ([\"#d4d4d4\"]                                         # KRR\n          + [\"#ffb3b3\", \"#ffc9a3\", \"#ffa3a3\", \"#ff8080\"]      # Row: softmax,cos,cay,ga\n          + [\"#a3c4ff\", \"#93b8ff\", \"#83acff\", \"#73a0ff\"])      # Col: softmax,cos,cay,ga\nfor patch, color in zip(bp[\"boxes\"], colors):\n    patch.set_facecolor(color)\nmeans = [sum(v)/len(v) for v in data]\nax1.scatter(range(1, len(means)+1), means, color=\"red\", zorder=5, s=40, marker=\"D\")\nax1.set_ylabel(\"MSE\")\nax1.set_title(\"All methods\", fontsize=12)\nax1.tick_params(axis='x', rotation=45)\n\n# Right: Paired bar chart — Row vs Col+Row per kernel\nimport numpy as np\nx = np.arange(len(row_kernels))\nwidth = 0.35\nrow_means = [sum(results[\"Row \" + k.capitalize()])/n_eval_tasks for k in row_kernels]\ncol_means = [sum(results[\"Col+\" + k.capitalize()])/n_eval_tasks for k in row_kernels]\nkrr_mean  = sum(results[\"KRR (RBF)\"]) / n_eval_tasks\n\nbars1 = ax2.bar(x - width/2, row_means, width, label=\"Row only\", color=\"coral\", alpha=0.85)\nbars2 = ax2.bar(x + width/2, col_means, width, label=\"Col+Row\", color=\"steelblue\", alpha=0.85)\nax2.axhline(krr_mean, color=\"gray\", ls=\"--\", lw=1.5, label=f\"KRR (RBF) = {krr_mean:.3f}\")\nax2.set_xticks(x)\nax2.set_xticklabels([k.capitalize() for k in row_kernels])\nax2.set_ylabel(\"Mean MSE\")\nax2.set_xlabel(\"Row kernel type\")\nax2.set_title(\"Column attention benefit per kernel\", fontsize=12)\nax2.legend(fontsize=9)\n\n# Add improvement % labels\nfor i, (r, c) in enumerate(zip(row_means, col_means)):\n    pct = 100 * (r - c) / r\n    ax2.annotate(f\"{pct:+.0f}%\", xy=(i + width/2, c), xytext=(0, 5),\n                 textcoords=\"offset points\", ha=\"center\", fontsize=9, fontweight=\"bold\",\n                 color=\"darkblue\")\n\nfig.suptitle(f\"ICL performance: Row-only vs Column+Row × kernel type (dim={input_dim})\", fontsize=13)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}